{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oUlztWyoDCs7",
        "outputId": "48057427-6b76-461f-971d-bbd27f6631e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>aspect</th>\n",
              "      <th>snippets</th>\n",
              "      <th>aspect_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Cinematography</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Direction</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Story</td>\n",
              "      <td>[The first thing that struck me about Oz was i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Characters</td>\n",
              "      <td>[Em City is home to many..Aryans, Muslims, gan...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Production Design</td>\n",
              "      <td>[]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345795</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Story</td>\n",
              "      <td>[The only problem I have with this movie, howe...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345796</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Characters</td>\n",
              "      <td>[The actors were amazing., Treat Williams is g...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345797</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Production Design</td>\n",
              "      <td>[]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345798</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Unique Concept</td>\n",
              "      <td>[]</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345799</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Emotions</td>\n",
              "      <td>[I remember loving it and everything about it....</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>345800 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review             aspect  \\\n",
              "0       One of the other reviewers has mentioned that ...     Cinematography   \n",
              "1       One of the other reviewers has mentioned that ...          Direction   \n",
              "2       One of the other reviewers has mentioned that ...              Story   \n",
              "3       One of the other reviewers has mentioned that ...         Characters   \n",
              "4       One of the other reviewers has mentioned that ...  Production Design   \n",
              "...                                                   ...                ...   \n",
              "345795  The first time I ever saw this movie was when ...              Story   \n",
              "345796  The first time I ever saw this movie was when ...         Characters   \n",
              "345797  The first time I ever saw this movie was when ...  Production Design   \n",
              "345798  The first time I ever saw this movie was when ...     Unique Concept   \n",
              "345799  The first time I ever saw this movie was when ...           Emotions   \n",
              "\n",
              "                                                 snippets  aspect_encoded  \n",
              "0                                                      []               0  \n",
              "1                                                      []               1  \n",
              "2       [The first thing that struck me about Oz was i...               2  \n",
              "3       [Em City is home to many..Aryans, Muslims, gan...               3  \n",
              "4                                                      []               4  \n",
              "...                                                   ...             ...  \n",
              "345795  [The only problem I have with this movie, howe...               2  \n",
              "345796  [The actors were amazing., Treat Williams is g...               3  \n",
              "345797                                                 []               4  \n",
              "345798                                                 []               5  \n",
              "345799  [I remember loving it and everything about it....               6  \n",
              "\n",
              "[345800 rows x 4 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "with open('../data/new_spans_labeled.json', 'r') as file:\n",
        "    dataset = json.load(file)\n",
        "\n",
        "# Aspects of interest\n",
        "aspects = ['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
        "\n",
        "# Aspect encoding mapping\n",
        "aspect_encoding = {aspect: index for index, aspect in enumerate(aspects)}\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "data = []\n",
        "\n",
        "for review in dataset:\n",
        "    review_text = review[\"review\"]\n",
        "    for aspect in aspects:\n",
        "        snippets = review.get(aspect, [])  # Get the snippets for the aspect, default to an empty list\n",
        "        if not snippets:  # Ensure a row even if there are no snippets\n",
        "            snippets = [\"\"]  # Use an empty string for consistency\n",
        "\n",
        "        # Combine snippets into a single list for each aspect\n",
        "        entry = {\n",
        "            \"review\": review_text,\n",
        "            \"aspect\": aspect,\n",
        "            \"snippets\": [snippet.replace('\"','') for snippet in snippets],  # Store snippets as a list\n",
        "            \"aspect_encoded\": aspect_encoding[aspect]\n",
        "        }\n",
        "        data.append(entry)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"../data/formatted_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VoD5soblDCs-"
      },
      "outputs": [],
      "source": [
        "new_rows=[]\n",
        "for index, row in df.iterrows():\n",
        "    snippets=[]\n",
        "    not_found=False\n",
        "    for snippet in row['snippets']:\n",
        "        if snippet!='':\n",
        "            if snippet.lower() in row['review'].lower():\n",
        "                snippets.append(snippet.lower())\n",
        "            else:\n",
        "                # print(snippet)\n",
        "                not_found=True\n",
        "                break\n",
        "    if not not_found:\n",
        "        new_rows.append([row['review'].lower(),row['aspect'],snippets])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fN6EL6G-DCs_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dC7Pxae9DCs_"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(new_rows, columns = ['review', 'aspect','snippet'])\n",
        "aspect_encoder = LabelEncoder()\n",
        "df['aspect_encoded'] = aspect_encoder.fit_transform(df['aspect'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-SQc2hjNf6b3"
      },
      "outputs": [],
      "source": [
        "aspect_counts = df['aspect'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snVoiyYAfmVv",
        "outputId": "c7371464-fe12-4bd8-fe4e-73bc14bcec83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aspect\n",
            "Production Design    20000\n",
            "Emotions             20000\n",
            "Cinematography       20000\n",
            "Direction            20000\n",
            "Unique Concept       20000\n",
            "Characters           20000\n",
            "Story                20000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "balanced_df = pd.DataFrame()\n",
        "\n",
        "for aspect in aspect_counts.index:\n",
        "    subset = df[df['aspect'] == aspect]\n",
        "    if len(subset) > 20000:\n",
        "        subset = subset.sample(n=20000, random_state=1)  # Random sampling\n",
        "    elif len(subset) < 20000:\n",
        "        # Duplicating entries if there are fewer than 20,000\n",
        "        subset = subset.sample(n=20000, replace=True, random_state=1)\n",
        "    balanced_df = pd.concat([balanced_df, subset])\n",
        "\n",
        "print(balanced_df['aspect'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T48YD2gsJkT2",
        "outputId": "c8b56159-6b5f-4984-852c-a6e65fcd5775"
      },
      "outputs": [],
      "source": [
        "# Define the dataset class\n",
        "class ReviewAspectDataset(Dataset):\n",
        "    def __init__(self, reviews, aspects, snippets, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.aspects = aspects\n",
        "        self.snippets = snippets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        aspect = str(self.aspects[idx])\n",
        "        snippets = self.snippets[idx]\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            aspect,\n",
        "            add_special_tokens=False,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        tokens = self.tokenizer.tokenize(review)\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        labels = [0] * len(token_ids)\n",
        "\n",
        "        for snippet in snippets:\n",
        "            snippet_tokens = self.tokenizer.tokenize(snippet)\n",
        "            snippet_token_ids = self.tokenizer.convert_tokens_to_ids(snippet_tokens)\n",
        "\n",
        "            for i in range(len(token_ids) - len(snippet_token_ids) + 1):\n",
        "                if token_ids[i:i+len(snippet_token_ids)] == snippet_token_ids:\n",
        "                    labels[i:i+len(snippet_token_ids)] = [1] * len(snippet_token_ids)\n",
        "                    break  # Assuming one occurrence of snippet in review\n",
        "\n",
        "        # Pad or truncate labels to match max_len\n",
        "        if len(labels) < self.max_len:\n",
        "            labels += [0] * (self.max_len - len(labels))\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "# Tokenizer and dataset preparation\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_len = 512\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oaQ1FWHnJl9N"
      },
      "outputs": [],
      "source": [
        "train_dataset = ReviewAspectDataset(\n",
        "    reviews=train_df['review'].to_numpy(),\n",
        "    aspects=train_df['aspect'].to_numpy(),\n",
        "    snippets=train_df['snippet'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "val_dataset = ReviewAspectDataset(\n",
        "    reviews=val_df['review'].to_numpy(),\n",
        "    aspects=val_df['aspect'].to_numpy(),\n",
        "    snippets=val_df['snippet'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hPrBikRGJsr8"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "import os\n",
        "from transformers import EarlyStoppingCallback\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    load_best_model_at_end=True,  # To ensure the best model is loaded at the end\n",
        "    save_total_limit=1,  # To keep only the best model\n",
        "    learning_rate= 5e-05\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q_YOZxWxJ0vN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 500/59314 [09:10<20:31:57,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1072, 'grad_norm': 0.5402780175209045, 'learning_rate': 5e-05, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      \n",
            "  1%|          | 500/59314 [1:01:37<20:31:57,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.07571308314800262, 'eval_runtime': 3147.1905, 'eval_samples_per_second': 18.847, 'eval_steps_per_second': 2.356, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1000/59314 [1:12:36<21:28:23,  1.33s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0736, 'grad_norm': 0.20583581924438477, 'learning_rate': 4.957493113884449e-05, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  2%|▏         | 1000/59314 [2:06:15<21:28:23,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0688648670911789, 'eval_runtime': 3219.6376, 'eval_samples_per_second': 18.423, 'eval_steps_per_second': 2.303, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 1500/59314 [2:17:24<20:41:03,  1.29s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0686, 'grad_norm': 0.2538255453109741, 'learning_rate': 4.914986227768899e-05, 'epoch': 0.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  3%|▎         | 1500/59314 [3:11:07<20:41:03,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.060462091118097305, 'eval_runtime': 3223.2685, 'eval_samples_per_second': 18.402, 'eval_steps_per_second': 2.3, 'epoch': 0.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 2000/59314 [3:21:58<21:18:06,  1.34s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0604, 'grad_norm': 0.26426786184310913, 'learning_rate': 4.872479341653348e-05, 'epoch': 0.07}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  3%|▎         | 2000/59314 [4:11:53<21:18:06,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.060468945652246475, 'eval_runtime': 2995.095, 'eval_samples_per_second': 19.804, 'eval_steps_per_second': 2.476, 'epoch': 0.07}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 2500/59314 [4:20:14<15:58:37,  1.01s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0614, 'grad_norm': 0.41730356216430664, 'learning_rate': 4.8299724555377975e-05, 'epoch': 0.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  4%|▍         | 2500/59314 [5:00:19<15:58:37,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.056259021162986755, 'eval_runtime': 2404.5265, 'eval_samples_per_second': 24.668, 'eval_steps_per_second': 3.084, 'epoch': 0.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 3000/59314 [5:08:40<15:39:24,  1.00s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0558, 'grad_norm': 0.8817415833473206, 'learning_rate': 4.7874655694222465e-05, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  5%|▌         | 3000/59314 [5:48:45<15:39:24,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0542791448533535, 'eval_runtime': 2404.8832, 'eval_samples_per_second': 24.664, 'eval_steps_per_second': 3.083, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 3500/59314 [5:57:07<15:46:28,  1.02s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0536, 'grad_norm': 0.533261239528656, 'learning_rate': 4.744958683306696e-05, 'epoch': 0.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  6%|▌         | 3500/59314 [6:34:51<15:46:28,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.05335463210940361, 'eval_runtime': 2263.9965, 'eval_samples_per_second': 26.199, 'eval_steps_per_second': 3.275, 'epoch': 0.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 4000/59314 [6:41:58<12:57:38,  1.19it/s]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0549, 'grad_norm': 0.5137315392494202, 'learning_rate': 4.702451797191145e-05, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  7%|▋         | 4000/59314 [7:17:52<12:57:38,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.05094146728515625, 'eval_runtime': 2153.5387, 'eval_samples_per_second': 27.543, 'eval_steps_per_second': 3.443, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 4500/59314 [7:27:38<20:46:05,  1.36s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.054, 'grad_norm': 0.3622477948665619, 'learning_rate': 4.659944911075594e-05, 'epoch': 0.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  8%|▊         | 4500/59314 [8:12:09<20:46:05,  1.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.051433391869068146, 'eval_runtime': 2671.5079, 'eval_samples_per_second': 22.202, 'eval_steps_per_second': 2.776, 'epoch': 0.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 5000/59314 [8:19:18<12:51:59,  1.17it/s]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0529, 'grad_norm': 0.25286808609962463, 'learning_rate': 4.617438024960044e-05, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  8%|▊         | 5000/59314 [8:52:05<12:51:59,  1.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.051521990448236465, 'eval_runtime': 1967.3512, 'eval_samples_per_second': 30.149, 'eval_steps_per_second': 3.769, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 5000/59314 [8:52:07<96:20:20,  6.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 31927.1848, 'train_samples_per_second': 14.862, 'train_steps_per_second': 1.858, 'train_loss': 0.06423976974487304, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.06423976974487304, metrics={'train_runtime': 31927.1848, 'train_samples_per_second': 14.862, 'train_steps_per_second': 1.858, 'total_flos': 1.045187026944e+16, 'train_loss': 0.06423976974487304, 'epoch': 0.1685942610513538})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import logging\n",
        "logging.disable(logging.WARNING)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cmqSX7JfgqY8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uvl_85K4gx0H"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7415/7415 [33:33<00:00,  3.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           O     0.9871    0.9973    0.9921  29752068\n",
            "   B-SNIPPET     0.7366    0.3690    0.4917    616700\n",
            "\n",
            "    accuracy                         0.9845  30368768\n",
            "   macro avg     0.8618    0.6831    0.7419  30368768\n",
            "weighted avg     0.9820    0.9845    0.9820  30368768\n",
            "\n",
            "Accuracy: 0.9845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.04379426687955856,\n",
              " 'eval_model_preparation_time': 0.0016,\n",
              " 'eval_accuracy': 0.9845067801235796,\n",
              " 'eval_f1': 0.7419,\n",
              " 'eval_precision': 0.6831499999999999,\n",
              " 'eval_recall': 0.86185,\n",
              " 'eval_runtime': 2014.0076,\n",
              " 'eval_samples_per_second': 29.451,\n",
              " 'eval_steps_per_second': 3.682}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# Custom evaluation function to generate classification report\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Flatten the predictions and labels\n",
        "    preds_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    # Filter out the labels where the label is -100 (ignore index)\n",
        "    mask = labels_flat != -100\n",
        "    preds_flat = preds_flat[mask]\n",
        "    labels_flat = labels_flat[mask]\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(labels_flat, preds_flat, target_names=['O', 'B-SNIPPET'], digits=4)\n",
        "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "\n",
        "    print(\"\\nClassification Report:\\n\", report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Safely extract metrics from the report\n",
        "    report_lines = report.split('\\n')\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    \n",
        "    # Extract metrics from each class, skipping the header and footer lines\n",
        "    for line in report_lines[2:-3]:\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 4:  # Ensure that the line has enough parts\n",
        "            recall_scores.append(float(parts[-4]))\n",
        "            precision_scores.append(float(parts[-3]))\n",
        "            f1_scores.append(float(parts[-2]))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": np.mean(f1_scores) if f1_scores else 0.0,\n",
        "        \"precision\": np.mean(precision_scores) if precision_scores else 0.0,\n",
        "        \"recall\": np.mean(recall_scores) if recall_scores else 0.0\n",
        "    }\n",
        "\n",
        "# Trainer with custom metrics\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Adding the custom compute_metrics function\n",
        ")\n",
        "\n",
        "# Evaluate the model on validation dataset\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8vSEwLXfeHZ2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./spans_based_bert_model/tokenizer_config.json',\n",
              " './spans_based_bert_model/special_tokens_map.json',\n",
              " './spans_based_bert_model/vocab.txt',\n",
              " './spans_based_bert_model/added_tokens.json')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Save the model\n",
        "model.save_pretrained('./spans_based_bert_model')\n",
        "tokenizer.save_pretrained('./spans_based_bert_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2Ram0_fpeIkC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "\n",
        "import re\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BertForTokenClassification.from_pretrained('./spans_based_bert_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('./spans_based_bert_model')\n",
        "\n",
        "def remove_specific_characters(strings_list):\n",
        "    # Define the characters to be removed\n",
        "    characters_to_remove = {\n",
        "    '\\x8d', '\\x8b', '\\x8c', '\\x8f', '\\x87', '\\x8e', '\\x81',\n",
        "    '\\x8a', '\\x83', '\\x94', '\\x95', '\\x97', '\\x91', '\\x89',\n",
        "    '\\x80', '\\x99', '\\x9e', '\\xad', '\\x9d', '\\x98', '\\x93',\n",
        "    '\\x82', '\\x9c', '\\x9f'\"®\", \"´\", \"¿\", \"¥\",\n",
        "        \"\\u00c3\", \"\\u00a2\", \"\\u00c2\", \"\\u0080\", \"\\u00c2\", \"\\u0099\"\n",
        "    }\n",
        "\n",
        "    cleaned_strings_list = []\n",
        "\n",
        "    for string in strings_list:\n",
        "        cleaned_string = ''.join(char for char in string if char not in characters_to_remove)\n",
        "        cleaned_strings_list.append(cleaned_string)\n",
        "\n",
        "    return cleaned_strings_list\n",
        "\n",
        "def remove_double_spaces(strings):\n",
        "    pattern = re.compile(r'\\s{2,}')  # Regex to match two or more spaces\n",
        "    return [pattern.sub(' ', text) for text in strings]\n",
        "\n",
        "def remove_multiple_punctuation(strings):\n",
        "    # Create patterns to find multiple occurrences of ., !, and ,\n",
        "    patterns = {\n",
        "        r'\\.{2,}': '.',\n",
        "        r'\\!{2,}': '!',\n",
        "        r'\\,{2,}': ','\n",
        "    }\n",
        "\n",
        "    # Process each string in the list\n",
        "    cleaned_strings = []\n",
        "    for text in strings:\n",
        "        for pattern, replacement in patterns.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "        cleaned_strings.append(text)\n",
        "\n",
        "    return cleaned_strings\n",
        "\n",
        "\n",
        "\n",
        "def predict_snippet(review, aspect, model, tokenizer, max_len=256):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        review,\n",
        "        aspect,\n",
        "        add_special_tokens=False,\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation='longest_first'\n",
        "    )\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=2).flatten().tolist()\n",
        "    new_predictions=predictions.copy()\n",
        "    # Decode the tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten().tolist())\n",
        "    # print(tokens)\n",
        "    snippets=[]\n",
        "    snippet=[]\n",
        "    i = 0\n",
        "    for token, label in zip(tokens, predictions):\n",
        "        if label == 1:\n",
        "            new_predictions[i] = 1\n",
        "            snippet.append(token)\n",
        "        elif label == 0 and i > 0 and i + 1 < len(tokens) and predictions[i - 1] == 1 and predictions[i + 1] == 1:\n",
        "            new_predictions[i] = 1\n",
        "            snippet.append(token)\n",
        "        elif len(snippet):\n",
        "            snippets.append(' '.join(snippet))\n",
        "            snippet = []\n",
        "        i += 1\n",
        "\n",
        "    for i in range(1, len(new_predictions) - 2):\n",
        "        # Check for the pattern 1,0,0,1\n",
        "        if new_predictions[i] == 0 and new_predictions[i+1] == 0 and new_predictions[i-1] == 1 and new_predictions[i+2] == 1:\n",
        "            new_predictions[i] = 1\n",
        "            new_predictions[i+1] = 1\n",
        "\n",
        "    # print(snippets)\n",
        "    return snippets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rj072SAUeNFi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original review:  the story was amazing but the cinematography wasn't it\n",
            "Cinematography [\"the cinematography wasn't\"]\n",
            "\n",
            "-------------------\n",
            "Direction []\n",
            "\n",
            "-------------------\n",
            "Story ['the story was amazing']\n",
            "\n",
            "-------------------\n",
            "Characters []\n",
            "\n",
            "-------------------\n",
            "Production Design []\n",
            "\n",
            "-------------------\n",
            "Unique Concept []\n",
            "\n",
            "-------------------\n",
            "Emotions []\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    original_review= remove_double_spaces([text])\n",
        "    original_review= remove_multiple_punctuation(original_review)\n",
        "    original_review = remove_specific_characters(original_review)[0]\n",
        "    text = original_review.lower()\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with a space\n",
        "    text = re.sub(r'\\.\\.+', '.', text)  # Replace multiple periods with a single period\n",
        "    text=text.replace(',','')\n",
        "    text=text.replace('.','')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
        "    return text\n",
        "\n",
        "def fix_special_characters(snippet):\n",
        "    snippet=snippet.replace(\"[UNK]\",'')\n",
        "    snippet=snippet.replace(\" ##\",'')\n",
        "    snippet=snippet.replace(\" '\",\"'\")\n",
        "    snippet=snippet.replace(\" ’\",\"’\")\n",
        "    snippet=snippet.replace(\"’ \",\"’\")\n",
        "    snippet=snippet.replace(\"' \",\"'\")\n",
        "    snippet=snippet.replace(\" -\",\"-\")\n",
        "    snippet=snippet.replace(\"- \",\"-\")\n",
        "    snippet=snippet.replace(\"/ \",\"/\")\n",
        "    snippet=snippet.replace(\" /\",\"/\")\n",
        "    snippet=snippet.replace(\" :\",\":\")\n",
        "    snippet=snippet.replace(\": \",\":\")\n",
        "    return snippet\n",
        "\n",
        "aspects=['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
        "original_review=\"\"\"The story was amazing but the cinematography wasn't it.\"\"\"\n",
        "\n",
        "#     print(aspect, new_snippets)\n",
        "original_review=clean_text(original_review)\n",
        "\n",
        "print(\"original review: \",original_review)\n",
        "\n",
        "for aspect in aspects:\n",
        "    predicted_snippets = predict_snippet(original_review, aspect, model, tokenizer)\n",
        "\n",
        "    new_snippets = []\n",
        "    for snippet in predicted_snippets:\n",
        "        new_snippets.append(fix_special_characters(snippet))\n",
        "\n",
        "    print(aspect, new_snippets, end='\\n')\n",
        "    print(\"\\n-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sequence': \"the cinematography wasn't great\", 'labels': ['cinematography negative', 'cinematography positive'], 'scores': [0.9993323683738708, 0.0006676383200101554]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\n",
        "\n",
        "# Example snippet\n",
        "snippet = \"the cinematography wasn't great\"\n",
        "candidate_labels = [\"cinematography positive\", \"cinematography negative\"]\n",
        "\n",
        "# Classify the sentiment\n",
        "sentiment_result = classifier(snippet, candidate_labels)\n",
        "print(sentiment_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
